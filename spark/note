1. run in localhost
pyspark --master local[*]

2. run pyspark in Hadoop YARN
HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop pyspark --master yarn --deploy-mode client

3.run pyspark in Spark standalone
/usr/local/spark/sbin/start-all.sh
pyspark --master spark://master:7077 --num-executors 1 --total-executor-cores 3 --executor-memory 1024m

4.run pyspark in Spark standalone by jupyter-notebook
sudo jupyter-notebook --no-browser --port=8888 --allow-root

#inpy
import pyspark
sc=pyspark.SparkContext(master="spark://master:7077")
sc.master

textFile=sc.textFile("hdfs://master:9000/user/ubuntu/wordcount/input/LICENSE.txt")
textFile.count()

# all web UI
1. hadoop cluster resource, control by YARN
http://{master ip}:8088

2. distribution file system (DFS) web UI
http://{master ip}:50070

3. Spark Master Web UI
http://{master ip}:8080
